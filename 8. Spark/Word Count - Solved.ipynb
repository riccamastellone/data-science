{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark | Word Count\n",
    "We will cover someSpark functions to build a word counter and test it on *La Divina Commedia* by Dante Alighieri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.13 (default, Dec 18 2016 07:03:39)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Import Spark\n",
    "# NOTE: This may differ depending on your system!\n",
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a base RDD and pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a base RDD\n",
    "We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList, 4) \n",
    "# 4 is the number of partition we are asking Spark to distribute out dataset\n",
    "\n",
    "print type(wordsRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pluralize \n",
    "\n",
    "Let's use a `map()` transformation to add the letter 's' to each string in the base RDD we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n"
     ]
    }
   ],
   "source": [
    "# One way of completing the function\n",
    "def makePlural(word):\n",
    "    return word + 's'\n",
    "\n",
    "print makePlural('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply `makePlural` to the base RDD\n",
    "\n",
    "Now pass each item in the base RDD into a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation that applies the `makePlural()` function to each element. And then call the [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) action to see the transformed RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralRDD = wordsRDD.map(makePlural)\n",
    "print pluralRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass a `lambda` function to `map`\n",
    "\n",
    "Let's create the same RDD using a `lambda` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats', 'elephants', 'rats', 'rats', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralLambdaRDD = wordsRDD.map(lambda x: x+'s')\n",
    "print pluralLambdaRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of each word\n",
    "\n",
    "Now use `map()` and a `lambda` function to return the number of characters in each word.  We'll `collect` this result directly into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "pluralLengths = (pluralRDD\n",
    "                 .map(lambda x: len(x))\n",
    "                 .collect())\n",
    "print pluralLengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDDs\n",
    "\n",
    "The next step in writing our word counting program is to create a new type of RDD, called a pair RDD. A pair RDD is an RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, we will create a pair consisting of `('<word>', 1)` for each word element in the RDD.\n",
    "We can create the pair RDD using the `map()` transformation with a `lambda()` function to create a new RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 1), ('elephant', 1), ('rat', 1), ('rat', 1), ('cat', 1)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordPairs = wordsRDD.map(lambda x: (x,1))\n",
    "print wordPairs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Counting with pair RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number of times a particular word appears in the RDD. There are multiple ways to perform the counting, but some are much less efficient than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupByKey()` approach\n",
    "An approach you might first consider (we'll see shortly that there are better ways) is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. As the name implies, the `groupByKey()` transformation groups all the elements of the RDD with the same key into a single list in one of the partitions.\n",
    "\n",
    "There are two problems with using `groupByKey()`:\n",
    "  + The operation requires a lot of data movement to move all the values into the appropriate partitions.\n",
    "  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory in a worker.\n",
    "\n",
    "Use `groupByKey()` to generate a pair RDD of type `('word', iterator)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rat: [1, 1]\n",
      "elephant: [1]\n",
      "cat: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that groupByKey requires no parameters\n",
    "wordsGrouped = wordPairs.groupByKey()\n",
    "for key, value in wordsGrouped.collect():\n",
    "    print '{0}: {1}'.format(key, list(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `groupByKey()` to obtain the counts\n",
    "\n",
    "Using the `groupByKey()` transformation creates an RDD containing 3 elements, each of which is a pair of a word and a Python iterator.\n",
    "\n",
    "Now sum the iterator using a `map()` transformation.  The result should be a pair RDD consisting of (word, count) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordCountsGrouped = wordsGrouped.map(lambda x: (x[0],sum(x[1])))\n",
    "print wordCountsGrouped.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting using `reduceByKey`\n",
    "\n",
    "A better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Note that reduceByKey takes in a function that accepts two values and returns a single value\n",
    "wordCounts = wordPairs.reduceByKey(lambda x,y: x+y)\n",
    "print wordCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All together\n",
    "\n",
    "The expert version of the code performs the `map()` to pair RDD, `reduceByKey()` transformation, and `collect` in one statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "wordCountsCollected = (wordsRDD\n",
    "                       .map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "                       .collect())\n",
    "print wordCountsCollected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Finding unique words and a mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique words\n",
    "\n",
    "Calculate the number of unique words in `wordsRDD`.  You can use other RDDs that you have already created to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "uniqueWords = wordCounts.count()\n",
    "print uniqueWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean using `reduce`\n",
    "\n",
    "Find the mean number of words per unique word in `wordCounts`.\n",
    "\n",
    "Use a `reduce()` action to sum the counts in `wordCounts` and then divide by the number of unique words.  First `map()` the pair RDD `wordCounts`, which consists of (key, value) pairs, to an RDD of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "from operator import add\n",
    "totalCount = (wordCounts\n",
    "              .map(lambda x: x[1])\n",
    "              .reduce(lambda x,y: x+y))\n",
    "average = totalCount / float(uniqueWords)\n",
    "print round(average, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert round(average, 2) == 1.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Apply word count to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will finish developing our word count application.  We'll have to build the `wordCount` function, deal with real world problems like capitalization and punctuation, load in our data source, and compute the word count on the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `wordCount` function\n",
    "\n",
    "First, define a function for word counting.  You should reuse the techniques that have been covered in earlier parts of this lab.  This function should take in an RDD that is a list of words like `wordsRDD` and return a pair RDD that has all of the words and their associated counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('rat', 2), ('elephant', 1), ('cat', 2)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def wordCount(wordListRDD):\n",
    "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
    "\n",
    "    Args:\n",
    "        wordListRDD (RDD of str): An RDD consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "    \"\"\"\n",
    "    return wordListRDD.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y)\n",
    "print wordCount(wordsRDD).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert sorted(wordCount(wordsRDD).collect()) == [('cat', 2), ('elephant', 1), ('rat', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization and punctuation\n",
    "\n",
    "Real world files are more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n",
    "  + Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n",
    "  + All punctuation should be removed.\n",
    "  + Any leading or trailing spaces on a line should be removed.\n",
    "\n",
    "Define the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use the Python [re](https://docs.python.org/2/library/re.html) module to remove any text that is not a letter, number, or space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi you\n",
      "no underscore\n",
      "remove punctuation then spaces\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "import re\n",
    "import string\n",
    "def removePunctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    pattern = re.compile('[%s]' % string.punctuation)\n",
    "    t =  re.sub(pattern, '', text.lower())\n",
    "    return re.sub(' +',' ',t).strip()\n",
    "\n",
    "print removePunctuation('Hi, you!')\n",
    "print removePunctuation(' No under_score!')\n",
    "print removePunctuation(' *      Remove punctuation then spaces  * ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert removePunctuation(\" The Elephant's   4 cats.  \") == 'the elephants 4 cats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a text file\n",
    "\n",
    "For the next part of this lab, we will use the La Divina Commedia by Dante Alighieri. To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `removePunctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lower case.  Since the file is large we use `take(15)`, so that we only print 15 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'la divina commedia', 0),\n",
       " (u'di dante alighieri', 1),\n",
       " (u'inferno', 2),\n",
       " (u'', 3),\n",
       " (u'', 4),\n",
       " (u'', 5),\n",
       " (u'inferno canto i', 6),\n",
       " (u'', 7),\n",
       " (u'nel mezzo del cammin di nostra vita', 8),\n",
       " (u'mi ritrovai per una selva oscura', 9),\n",
       " (u'ch\\xe9 la diritta via era smarrita', 10),\n",
       " (u'ahi quanto a dir qual era \\xe8 cosa dura', 11),\n",
       " (u'esta selva selvaggia e aspra e forte', 12),\n",
       " (u'che nel pensier rinova la paura', 13),\n",
       " (u'tant\\xe8 amara che poco \\xe8 pi\\xf9 morte', 14)]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "danteRDD = sc.textFile('input/divina_commedia.txt').map(removePunctuation)\n",
    "danteRDD.zipWithIndex().take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words from lines\n",
    "\n",
    "Before we can use the `wordcount()` function, we have to address two issues with the format of the RDD:\n",
    "  + The first issue is that  that we need to split each line by its spaces.\n",
    "  + The second issue is we need to filter out empty lines.\n",
    "\n",
    "Apply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply Python's string [split()](https://docs.python.org/2/library/string.html#string.split) function. You might think that a `map()` transformation is the way to do this, but think about what the result of the `split()` function will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96972\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "danteWordsRDD = danteRDD.flatMap(lambda x: x.split(' '))\n",
    "danteWordCount = danteWordsRDD.count()\n",
    "print danteWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert danteWordCount == 96972"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Remove empty elements **\n",
    "\n",
    "The next step is to filter out the empty elements.  Remove all entries where the word is `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96561\n"
     ]
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "danteWordsRDD2 = danteWordsRDD.filter(lambda x: len(x) > 0)\n",
    "danteWordsCount2 = danteWordsRDD2.count()\n",
    "print danteWordsCount2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert danteWordsCount2 == 96561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the words\n",
    "\n",
    "We now have an RDD that is only words.  Next, let's apply the `wordCount()` function to produce a list of word counts. We can view the top 25 words by using the `takeOrdered()` action; however, since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair.\n",
    "\n",
    "You'll notice that many of the words are common English words. These are called stopwords. In a later lab, we will see how to eliminate them from the results.\n",
    "Use the `wordCount()` function and `takeOrdered()` to obtain the fifteen most common words and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 4038\n",
      "che: 3716\n",
      "la: 2361\n",
      "di: 1970\n",
      "a: 1899\n",
      "non: 1456\n",
      "per: 1384\n",
      "in: 1103\n",
      "si: 1060\n",
      "l: 951\n",
      "le: 801\n",
      "sì: 796\n",
      "li: 781\n",
      "mi: 741\n",
      "il: 670\n",
      "più: 660\n",
      "de: 659\n",
      "come: 648\n",
      "con: 645\n",
      "da: 642\n",
      "lo: 591\n",
      "del: 569\n",
      "se: 559\n",
      "è: 542\n",
      "ma: 508\n"
     ]
    }
   ],
   "source": [
    "top25WordsAndCounts = danteWordsRDD2.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).takeOrdered(25, lambda s: -s[1])\n",
    "print '\\n'.join(map(lambda (w, c): u'{0}: {1}'.format(w, c), top25WordsAndCounts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "name": "cs120_lab1b_word_count_rdd",
  "notebookId": 4099732974528733
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
