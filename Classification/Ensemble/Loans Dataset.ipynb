{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ensembles Classifiers using Trees on the Loan Dataset\n",
    "\n",
    "In this notebook we apply several ensemble methods to the loan dataset we used in other examples.\n",
    "\n",
    "First we load all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constructs a new estimator with the same parameters.\n",
    "# Clone does a deep copy of the model in an estimator \n",
    "# without actually copying attached data. It yields a \n",
    "# new estimator with the same parameters that has not \n",
    "# been fit on any data.\n",
    "from sklearn import clone \n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,BaggingClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we define some of the parameters required to run the experiments like the number of estimators used in each ensembles and the random seed to be able to reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of estimators used in each ensemble\n",
    "n_estimators = 30\n",
    "\n",
    "# Set the random seed to be able to repeat the experiment\n",
    "random_seed = 42 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the loans dataset, define the variable $y$, and the input variables $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('LoansNumerical.csv')\n",
    "target = 'safe_loans'\n",
    "features = loans.columns[loans.columns!=target]\n",
    "\n",
    "x = loans[features]\n",
    "y = loans[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set the models to be compared\n",
    "- Simple Decision Tree\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "models = {'Decision Tree':DecisionTreeClassifier(),\n",
    "          'Bagging':BaggingClassifier(DecisionTreeClassifier(),n_estimators=n_estimators),\n",
    "          'Random Forest':RandomForestClassifier(n_estimators=n_estimators),\n",
    "          'Ada Boost':AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=n_estimators)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For each model, we apply 10-fold stratified crossvalidation and compute the average accuracy and the corresponding standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t                   Bagging\t0.814 +/- 0.001\n",
      "\t                 Ada Boost\t0.728 +/- 0.002\n",
      "\t             Random Forest\t0.817 +/- 0.001\n",
      "\t             Decision Tree\t0.728 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_seed)\n",
    "scores = {}\n",
    "for model_name in models:\n",
    "    clf = models[model_name];\n",
    "    score = cross_val_score(clf,x,y,cv=folds)\n",
    "    scores[model_name]=(np.average(score),np.std(score))\n",
    "    print('\\t%26s\\t%.3f +/- %.3f'%(model_name,scores[model_name][0],scores[model_name][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance\n",
    "Ensembles generate models that are difficult to analyze but provide interesting ways to score the variable used by all the models in the ensembles. First, we fit each ensemble,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fitted = {}\n",
    "for model_name in models:\n",
    "    model = clone(models[model_name])\n",
    "    score = model.fit(x,y)\n",
    "    fitted[model_name] = (model,score,model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the variable importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_idx = 1\n",
    "plt.figure(figsize=(10,20))\n",
    "for model_name in fitted:\n",
    "    plt.subplot(4,1,plot_idx)\n",
    "    importances = fitted[model_name][2]\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Plot the feature importances \n",
    "    plt.title(\"Feature importances - \"+model_name)\n",
    "    plt.xticks(range(x.shape[1]),x.columns[indices],rotation='vertical')\n",
    "    \n",
    "    plt.bar(range(x.shape[1]), importances[indices],align=\"center\")\n",
    "    plot_idx = plot_idx + 1\n",
    "\n",
    "plt.tight_layout(pad=1.08, h_pad=None, w_pad=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble using k-nearest neighbors\n",
    "We now applt Bagging using k-nearest neighbors as the base classifier. Note that we use only Baggin and Ada Boost since Random Forest and Extra Trees are bound to decision trees; we cannot use Ada Boost since the k-nearest-neighbor implementation does not support the sample weights that are needed by Ada Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "knn_models = {'k-Nearest Neighbors':KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree'),\n",
    "          'Bagging':BaggingClassifier(KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree'),n_estimators=n_estimators)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_seed)\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for model_name in knn_models:\n",
    "    clf = clone(knn_models[model_name]);\n",
    "    score = cross_val_score(clf,x,y,cv=folds)\n",
    "    scores[model_name]=(np.average(score),np.std(score))\n",
    "    print('\\t%26s\\t%.3f +/- %.3f'%(model_name,scores[model_name][0],scores[model_name][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifiers\n",
    "We can build our own ensembles by computing a set of heterogeneous models and then, to compute a class, first we compute the output of all the models and use the majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "        ('k-Nearest Neighbors', KNeighborsClassifier(n_neighbors=16, algorithm='kd_tree')), \n",
    "        ('Decision Tree', DecisionTreeClassifier(max_depth=None)), \n",
    "        ('Logistic Regression', LogisticRegression(random_state=random_seed)),\n",
    "        ('Bagging', BaggingClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=n_estimators)),\n",
    "        ('Random Forest',RandomForestClassifier(n_estimators=n_estimators)),\n",
    "        ('Ada Boost',AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=n_estimators))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (label,model) in estimators:\n",
    "    scores = cross_val_score(clone(model), x, y, cv=folds, scoring='accuracy')\n",
    "    print('\\t%26s\\t%.3f +/- %.3f'%(label,np.average(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "voting = VotingClassifier(estimators=estimators)\n",
    "scores = cross_val_score(voting, x, y, cv=folds, scoring='accuracy')\n",
    "print('\\t%26s\\t%.3f +/- %.3f'%('Voting (soft)',np.average(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
