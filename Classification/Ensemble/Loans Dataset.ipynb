{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Ensembles Classifiers using Trees on the Loan Dataset\n",
    "\n",
    "In this notebook we apply several ensemble methods to the loan dataset we used in other examples.\n",
    "\n",
    "First we load all the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier,BaggingClassifier)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next we define some of the parameters required to run the experiments like the number of estimators used in each ensembles and the random seed to be able to reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of estimators used in each ensemble\n",
    "n_estimators = 30\n",
    "\n",
    "# set the random seed to be able to repeat the experiment\n",
    "random_seed = 1234 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load the loans dataset, define the variable $y$, and the input variables $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "loans = pd.read_csv('LoansNumerical.csv')\n",
    "target = 'safe_loans'\n",
    "features = loans.columns[loans.columns!=target]\n",
    "\n",
    "x = loans[features]\n",
    "y = loans[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set the models to be compared\n",
    "- simple decision tree\n",
    "- bagging\n",
    "- random forest\n",
    "- extra tree classifiers\n",
    "- adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "models = {'Decision Tree':DecisionTreeClassifier(max_depth=None),\n",
    "          'Bagging':BaggingClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=n_estimators),\n",
    "          'Random Forest':RandomForestClassifier(n_estimators=n_estimators),\n",
    "          'Extremely Randomized Trees':ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "          'Ada Boost':AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                             n_estimators=n_estimators)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "For each model, we apply 10-fold stratified crossvalidation and compute the average accuracy and the corresponding standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t                   Bagging\t0.815 +/- 0.001\n",
      "\tExtremely Randomized Trees\t0.813 +/- 0.002\n",
      "\t                 Ada Boost\t0.821 +/- 0.002\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits=10,shuffle=True,random_state=random_seed)\n",
    "scores = {}\n",
    "for model_name in models:\n",
    "    clf = models[model_name];\n",
    "    score = cross_val_score(clf,x,y,cv=folds)\n",
    "    scores[model_name]=(np.average(score),np.std(score))\n",
    "    print('\\t%26s\\t%.3f +/- %.3f'%(model_name,scores[model_name][0],scores[model_name][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance\n",
    "Ensembles generate models that are difficult to analyze but provide interesting ways to score the variable used by all the models in the ensembles. First, we fit each ensemble,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fitted = {}\n",
    "for model_name in models:\n",
    "    model = clone(models[model_name])\n",
    "    score = model.fit(x,y)\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        fitted[model_name] = (model,score,model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we plot the variable importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_idx = 1\n",
    "\n",
    "font = {'family' : 'sans', 'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for model_name in fitted:\n",
    "    model = fitted[model_name][0]\n",
    "    \n",
    "    # importance of features\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # indeces of the variables\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    plt.subplot(4,1,plot_idx)\n",
    "    \n",
    "    # \n",
    "    plt.title(\"Feature importances - \"+model_name)\n",
    "    plt.xticks(range(x.shape[1]),x.columns[indices],rotation='vertical')\n",
    "\n",
    "\n",
    "    plt.xlim([-1, x.shape[1]])\n",
    "\n",
    "    if hasattr(model, 'estimators_'):\n",
    "        std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "        plt.bar(range(x.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")# plt.xticks(range(X.shape[1]), indices)\n",
    "    else:\n",
    "        plt.bar(range(x.shape[1]), importances[indices], color=\"r\")\n",
    "\n",
    "\n",
    "    plot_idx = plot_idx + 1\n",
    "    \n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble using k-nearest neighbors\n",
    "We now applt Bagging using k-nearest neighbors as the base classifier. Note that we use only Baggin and Ada Boost since Random Forest and Extra Trees are bound to decision trees; we cannot use Ada Boost since the k-nearest-neighbor implementation does not support the sample weights that are needed by Ada Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 16\n",
    "knn_models = {'k-Nearest Neighbors':KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree'),\n",
    "          'Bagging':BaggingClassifier(KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree'),n_estimators=n_estimators)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=10,shuffle=True,random_state=random_seed)\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for model_name in knn_models:\n",
    "    clf = clone(knn_models[model_name]);\n",
    "    score = cross_val_score(clf,x,y,cv=folds)\n",
    "    scores[model_name]=(np.average(score),np.std(score))\n",
    "    print('\\t%26s\\t%.3f +/- %.3f'%(model_name,scores[model_name][0],scores[model_name][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifiers\n",
    "We can build our own ensembles by computing a set of heterogeneous models and then, to compute a class, first we compute the output of all the models and use the majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "        ('k-Nearest Neighbors', KNeighborsClassifier(n_neighbors=16, algorithm='kd_tree')), \n",
    "        ('Decision Tree', DecisionTreeClassifier(max_depth=None)), \n",
    "        ('Logistic (Regularized)', LogisticRegression(random_state=random_seed)),\n",
    "        ('Logistic Regression', LogisticRegression(random_state=random_seed, C=10e6)),\n",
    "        ('Bagging', BaggingClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=n_estimators)),\n",
    "        ('Random Forest',RandomForestClassifier(n_estimators=n_estimators)),\n",
    "        ('Extremely Randomized Trees',ExtraTreesClassifier(n_estimators=n_estimators)),\n",
    "        ('Ada Boost',AdaBoostClassifier(DecisionTreeClassifier(max_depth=3), n_estimators=n_estimators)),\n",
    "        ('GNB', GaussianNB())\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_hard_voting = VotingClassifier(estimators=estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_soft_voting = VotingClassifier(estimators=estimators, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for (label,model) in estimators:\n",
    "    scores = cross_val_score(clone(model), x, y, cv=folds, scoring='accuracy')\n",
    "    print('\\t%26s\\t%.3f +/- %.3f'%(label,np.average(scores),np.std(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hard_scores = cross_val_score(ensemble_hard_voting, x, y, cv=folds, scoring='accuracy')\n",
    "soft_scores = cross_val_score(ensemble_soft_voting, x, y, cv=folds, scoring='accuracy')\n",
    "print('\\t%26s\\t%.3f +/- %.3f'%('Voting (hard)',np.average(hard_scores),np.std(hard_scores)))\n",
    "print('\\t%26s\\t%.3f +/- %.3f'%('Voting (soft)',np.average(soft_scores),np.std(soft_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
