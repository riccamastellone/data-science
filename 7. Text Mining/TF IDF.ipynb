{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "\n",
    "Bag-of-words comparisons are not very good when all tokens are treated the same: some tokens are more important than others. Weights give us a way to specify which tokens to favor. With weights, when we compare documents, instead of counting common tokens, we sum up the weights of common tokens. A good heuristic for assigning weights is called \"Term-Frequency/Inverse-Document-Frequency,\" or tf-idf for short.\n",
    "\n",
    "We will build and test our TF-IDF model using the data from the Quora challenge on Kaggle (https://www.kaggle.com/c/quora-question-pairs) were the goal was to identify the duplicate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF\n",
    "\n",
    "TF rewards tokens that appear many times in the same document. It is computed as the frequency of a token in a document, that is, if document *d* contains 100 tokens and token *t* appears in *d* 5 times, then the TF weight of *t* in *d* is *5/100 = 1/20*. The intuition for TF is that if a word occurs often in a document, then it is more important to the meaning of the document.\n",
    "\n",
    "### IDF\n",
    "\n",
    "IDF rewards tokens that are rare overall in a dataset. The intuition is that it is more significant if two documents share a rare word than a common one. IDF weight for a token, *t*, in a set of documents, *U*, is computed as follows:\n",
    "* Let *N* be the total number of documents in *U*\n",
    "* Find *n(t)*, the number of documents in *U* that contain *t*\n",
    "* Then *IDF(t) = N/n(t)*.\n",
    "\n",
    "Note that *n(t)/N* is the frequency of *t* in *U*, and *N/n(t)* is the inverse frequency.\n",
    "\n",
    "> **Note on terminology**: Sometimes token weights depend on the document the token belongs to, that is, the same token may have a different weight when it's found in different documents.  We call these weights *local* weights.  TF is an example of a local weight, because it depends on the length of the source.  On the other hand, some token weights only depend on the token, and are the same everywhere that token is found.  We call these weights *global*, and IDF is one such weight.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "Finally, to bring it all together, the total TF-IDF weight for a token in a document is the product of its TF and IDF weights.\n",
    "[tfidf]: https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import dataset and stop words\n",
    "dataset = pd.read_csv(\"input/train_quora.csv\", sep = ',')\n",
    "dataset = dataset[:1000] # just a sample in order to speed up the computation\n",
    "\n",
    "### Stop Words Dictionary ###\n",
    "f = open(\"input/english-stop-words-large.txt\",'r')\n",
    "stop_words_raw = f.readlines()\n",
    "stop_words = [i.replace('\\n',\"\") for i in stop_words_raw]\n",
    "\n",
    "split_regex = r'\\W+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simpleTokenize(string):\n",
    "    \"\"\" A simple implementation of input string tokenization\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens\n",
    "    \"\"\"\n",
    "    x = re.split(split_regex,string.lower())\n",
    "    return [i for i in x if i != '']\n",
    "\n",
    "def tokenize(string):\n",
    "    \"\"\" An implementation of input string tokenization that excludes stopwords\n",
    "    Args:\n",
    "        string (str): input string\n",
    "    Returns:\n",
    "        list: a list of tokens without stopwords\n",
    "    \"\"\"\n",
    "    x = simpleTokenize(string)\n",
    "    return [i for i in x if i not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the dataset columnwise\n",
    "dataset[\"question1_token\"] = dataset.apply(lambda row : tokenize(row[\"question1\"]),axis = 1)\n",
    "dataset[\"question2_token\"] = dataset.apply(lambda row : tokenize(row[\"question2\"]),axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'brown': 0.16666666666666666, 'lazy': 0.16666666666666666, 'jumps': 0.16666666666666666, 'fox': 0.16666666666666666, 'dog': 0.16666666666666666, 'quick': 0.16666666666666666}\n"
     ]
    }
   ],
   "source": [
    "def countTokens(df,q):\n",
    "    \"\"\" Count and return the number of tokens\n",
    "    Args:\n",
    "        dataframe, question column name\n",
    "    Returns:\n",
    "        count: count of all tokens\n",
    "    \"\"\"\n",
    "    return df.apply(lambda row: len(row[q]), axis = 1).sum()\n",
    "\n",
    "def tf(tokens):\n",
    "    \"\"\" Compute TF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "    Returns:\n",
    "        dictionary: a dictionary of tokens to its TF values\n",
    "    \"\"\"\n",
    "    dictionary = {}\n",
    "    for i in tokens:\n",
    "      dictionary[i] = tokens.count(i)/float(len(tokens))\n",
    "    return dictionary\n",
    "\n",
    "# Let's see the result on a famous sentence often employed in text analytics\n",
    "print tf(tokenize(\"The quick brown fox jumps over the lazy dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the corpus with all the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus1 = pd.DataFrame()\n",
    "corpus2 = pd.DataFrame()\n",
    "corpus1[\"id\"] = dataset[['qid1']]\n",
    "corpus1[\"tf\"] = dataset[['qid1',\"question1_token\"]].apply(lambda row: tf(row[\"question1_token\"]), axis = 1)\n",
    "corpus2[\"id\"] = dataset[['qid2']]\n",
    "corpus2[\"tf\"] = dataset[['qid2',\"question2_token\"]].apply(lambda row: tf(row[\"question2_token\"]), axis = 1)\n",
    "corpus = pd.concat([corpus1,corpus2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idfs(corpus):\n",
    "    \"\"\" Compute IDF\n",
    "    Args:\n",
    "        corpus dataframe\n",
    "    Returns:\n",
    "        dictionary of {tokens:idf}\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(corpus)\n",
    "    tokens = [item for sublist in list(corpus.apply(lambda row: row[\"tf\"].keys(),axis = 1)) for item in sublist]\n",
    "    unique_tokens = list(set(tokens))\n",
    "    idf_dict = {}\n",
    "    for tok in unique_tokens:\n",
    "        count = 0\n",
    "        for doc in corpus.tf:\n",
    "            if tok in doc.keys():\n",
    "                count += 1\n",
    "                continue\n",
    "        idf_dict[tok] = N/float(count)\n",
    "\n",
    "    return idf_dict\n",
    "\n",
    "idf_glob = idfs(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(tokens, idfs):\n",
    "    \"\"\" Compute TF-IDF\n",
    "    Args:\n",
    "        tokens (list of str): input list of tokens from tokenize\n",
    "        idfs (dictionary): record to IDF value\n",
    "    Returns:\n",
    "        dictionary: a dictionary of records to TF-IDF values\n",
    "    \"\"\"\n",
    "    tfs = tf(tokens)\n",
    "    tfIdfDict = {k: v*idfs[k] for (k,v) in tfs.items()}\n",
    "    return tfIdfDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity\n",
    "Now we are ready to do text comparisons in a formal way. The metric of string distance we will use is called **[cosine similarity][cosine]**. We will treat each document as a vector in some high dimensional space. Then, to compare two documents we compute the cosine of the angle between their two document vectors. This is *much* easier than it sounds.\n",
    "\n",
    "The first question to answer is how do we represent documents as vectors? The answer is familiar: bag-of-words! We treat each unique token as a dimension, and treat token weights as magnitudes in their respective token dimensions. For example, suppose we use simple counts as weights, and we want to interpret the string \"Hello, world!  Goodbye, world!\" as a vector. Then in the \"hello\" and \"goodbye\" dimensions the vector has value 1, in the \"world\" dimension it has value 2, and it is zero in all other dimensions.\n",
    "\n",
    "The next question is: given two vectors how do we find the cosine of the angle between them? Recall the formula for the dot product of two vectors:\n",
    "\\\\[ a \\cdot b = \\| a \\| \\| b \\| \\cos \\theta \\\\]\n",
    "Here \\\\( a \\cdot b = \\sum a_i b_i \\\\) is the ordinary dot product of two vectors, and \\\\( \\|a\\| = \\sqrt{ \\sum a_i^2 } \\\\) is the norm of \\\\( a \\\\).\n",
    "\n",
    "We can rearrange terms and solve for the cosine to find it is simply the normalized dot product of the vectors. With our vector model, the dot product and norm computations are simple functions of the bag-of-words document representations, so we now have a formal way to compute similarity:\n",
    "\\\\[ similarity = \\cos \\theta = \\frac{a \\cdot b}{\\|a\\| \\|b\\|} = \\frac{\\sum a_i b_i}{\\sqrt{\\sum a_i^2} \\sqrt{\\sum b_i^2}} \\\\]\n",
    "\n",
    "Setting aside the algebra, the geometric interpretation is more intuitive. The angle between two document vectors is small if they share many tokens in common, because they are pointing in roughly the same direction. For that case, the cosine of the angle will be large. Otherwise, if the angle is large (and they have few words in common), the cosine is small. Therefore, cosine similarity scales proportionally with our intuitive sense of similarity.\n",
    "[cosine]: https://en.wikipedia.org/wiki/Cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 6.16441400297 0.826297021229\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "##### COSINE SIMILARITY ####\n",
    "\n",
    "def dotprod(a, b):\n",
    "    \"\"\" Compute dot product\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        dotProd: result of the dot product with the two input dictionaries\n",
    "    \"\"\"\n",
    "    common_key = list(set(a.keys()).intersection(set(b.keys())))\n",
    "    return sum([a[i]*b[i] for i in common_key])\n",
    "\n",
    "def norm(a):\n",
    "    \"\"\" Compute square root of the dot product\n",
    "    Args:\n",
    "        a (dictionary): a dictionary of record to value\n",
    "    Returns:\n",
    "        norm (float): the square root of the dot product value\n",
    "    \"\"\"\n",
    "    return math.sqrt(sum([i**2 for i in a.values()]))\n",
    "\n",
    "def cossim(a, b):\n",
    "    \"\"\" Compute cosine similarity\n",
    "    Args:\n",
    "        a (dictionary): first dictionary of record to value\n",
    "        b (dictionary): second dictionary of record to value\n",
    "    Returns:\n",
    "        cossim: dot product of two dictionaries divided by the norm of the first dictionary and\n",
    "                then by the norm of the second dictionary\n",
    "    \"\"\"\n",
    "    return dotprod(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "testVec1 = {'foo': 2, 'bar': 3, 'baz': 5 }\n",
    "testVec2 = {'foo': 1, 'bar': 0, 'baz': 20 }\n",
    "dp = dotprod(testVec1, testVec2)\n",
    "nm = norm(testVec1)\n",
    "cs = cossim(testVec1, testVec2)\n",
    "print dp, nm, cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosineSimilarity(string1, string2, idfsDictionary):\n",
    "    \"\"\" Compute cosine similarity between two strings\n",
    "    Args:\n",
    "        string1 (str): first string\n",
    "        string2 (str): second string\n",
    "        idfsDictionary (dictionary): a dictionary of IDF values\n",
    "    Returns:\n",
    "        cossim: cosine similarity value\n",
    "    \"\"\"\n",
    "    w1 = tfidf(tokenize(string1),idfsDictionary)\n",
    "    w2 = tfidf(tokenize(string2),idfsDictionary)\n",
    "    return cossim(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[\"flag_len\"] = dataset.apply(lambda row: 1 if (len(row[\"question1_token\"])==0 or  len(row[\"question2_token\"])==0) else 0, axis = 1) \n",
    "\n",
    "dataset = dataset[dataset.flag_len == 0]\n",
    "\n",
    "dataset[\"cos_sim\"] = dataset.apply(lambda row: cosineSimilarity(row[\"question1\"], row[\"question2\"], idf_glob), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity == Duplicate?\n",
    "As you may notice in the two tables printed below, some additional tuning is required to better understand when two questions are a duplicate, mainly (a) fine tune the stop words, as in the provided dictionary there are probably too many, and (b) build a synonyms table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_token</th>\n",
       "      <th>question2_token</th>\n",
       "      <th>flag_len</th>\n",
       "      <th>cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>507</td>\n",
       "      <td>508</td>\n",
       "      <td>What are the qualities of a good leader?</td>\n",
       "      <td>What are some good qualities of a leader?</td>\n",
       "      <td>1</td>\n",
       "      <td>[qualities, good, leader]</td>\n",
       "      <td>[good, qualities, leader]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>568</td>\n",
       "      <td>1134</td>\n",
       "      <td>1135</td>\n",
       "      <td>How do I start writing again?</td>\n",
       "      <td>How do I start writing?</td>\n",
       "      <td>0</td>\n",
       "      <td>[start, writing]</td>\n",
       "      <td>[start, writing]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>307</td>\n",
       "      <td>308</td>\n",
       "      <td>At what age should someone lose their virginity?</td>\n",
       "      <td>At what age, how, and where did you lose your ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[age, lose, virginity]</td>\n",
       "      <td>[age, lose, virginity]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>922</th>\n",
       "      <td>922</td>\n",
       "      <td>1839</td>\n",
       "      <td>1840</td>\n",
       "      <td>How should I start meditating?</td>\n",
       "      <td>How should I start meditating, and when?</td>\n",
       "      <td>1</td>\n",
       "      <td>[start, meditating]</td>\n",
       "      <td>[start, meditating]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>222</td>\n",
       "      <td>445</td>\n",
       "      <td>446</td>\n",
       "      <td>How can I find job in Japan?</td>\n",
       "      <td>How can I find an IT job in Japan?</td>\n",
       "      <td>0</td>\n",
       "      <td>[find, job, japan]</td>\n",
       "      <td>[find, job, japan]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>277</td>\n",
       "      <td>554</td>\n",
       "      <td>555</td>\n",
       "      <td>How do most people die?</td>\n",
       "      <td>How do people die?</td>\n",
       "      <td>0</td>\n",
       "      <td>[people, die]</td>\n",
       "      <td>[people, die]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>291</td>\n",
       "      <td>582</td>\n",
       "      <td>583</td>\n",
       "      <td>Is there an end to the universe, and if not, i...</td>\n",
       "      <td>Is the Universe infinite or is there an end to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[end, universe, universe, infinite]</td>\n",
       "      <td>[universe, infinite, end, universe]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>215</td>\n",
       "      <td>216</td>\n",
       "      <td>What's the difference between love and pity?</td>\n",
       "      <td>What is the difference between love and pity?</td>\n",
       "      <td>1</td>\n",
       "      <td>[difference, love, pity]</td>\n",
       "      <td>[difference, love, pity]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>609</td>\n",
       "      <td>1216</td>\n",
       "      <td>1217</td>\n",
       "      <td>How can eating prunes help during a constipation?</td>\n",
       "      <td>Does eating prunes help with constipation?</td>\n",
       "      <td>1</td>\n",
       "      <td>[eating, prunes, constipation]</td>\n",
       "      <td>[eating, prunes, constipation]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601</td>\n",
       "      <td>1200</td>\n",
       "      <td>1201</td>\n",
       "      <td>What should I do for Web design?</td>\n",
       "      <td>What Is Web Design?</td>\n",
       "      <td>0</td>\n",
       "      <td>[web, design]</td>\n",
       "      <td>[web, design]</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  qid1  qid2                                          question1  \\\n",
       "253  253   507   508           What are the qualities of a good leader?   \n",
       "568  568  1134  1135                      How do I start writing again?   \n",
       "153  153   307   308   At what age should someone lose their virginity?   \n",
       "922  922  1839  1840                     How should I start meditating?   \n",
       "222  222   445   446                       How can I find job in Japan?   \n",
       "277  277   554   555                            How do most people die?   \n",
       "291  291   582   583  Is there an end to the universe, and if not, i...   \n",
       "107  107   215   216       What's the difference between love and pity?   \n",
       "609  609  1216  1217  How can eating prunes help during a constipation?   \n",
       "601  601  1200  1201                   What should I do for Web design?   \n",
       "\n",
       "                                             question2  is_duplicate  \\\n",
       "253          What are some good qualities of a leader?             1   \n",
       "568                            How do I start writing?             0   \n",
       "153  At what age, how, and where did you lose your ...             0   \n",
       "922           How should I start meditating, and when?             1   \n",
       "222                 How can I find an IT job in Japan?             0   \n",
       "277                                 How do people die?             0   \n",
       "291  Is the Universe infinite or is there an end to...             1   \n",
       "107      What is the difference between love and pity?             1   \n",
       "609         Does eating prunes help with constipation?             1   \n",
       "601                                What Is Web Design?             0   \n",
       "\n",
       "                         question1_token                      question2_token  \\\n",
       "253            [qualities, good, leader]            [good, qualities, leader]   \n",
       "568                     [start, writing]                     [start, writing]   \n",
       "153               [age, lose, virginity]               [age, lose, virginity]   \n",
       "922                  [start, meditating]                  [start, meditating]   \n",
       "222                   [find, job, japan]                   [find, job, japan]   \n",
       "277                        [people, die]                        [people, die]   \n",
       "291  [end, universe, universe, infinite]  [universe, infinite, end, universe]   \n",
       "107             [difference, love, pity]             [difference, love, pity]   \n",
       "609       [eating, prunes, constipation]       [eating, prunes, constipation]   \n",
       "601                        [web, design]                        [web, design]   \n",
       "\n",
       "     flag_len  cos_sim  \n",
       "253         0      1.0  \n",
       "568         0      1.0  \n",
       "153         0      1.0  \n",
       "922         0      1.0  \n",
       "222         0      1.0  \n",
       "277         0      1.0  \n",
       "291         0      1.0  \n",
       "107         0      1.0  \n",
       "609         0      1.0  \n",
       "601         0      1.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sort_values(by='cos_sim', ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_token</th>\n",
       "      <th>question2_token</th>\n",
       "      <th>flag_len</th>\n",
       "      <th>cos_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>What is a good song for lyric prank?</td>\n",
       "      <td>Diving the Blue Hole in Dahab?</td>\n",
       "      <td>0</td>\n",
       "      <td>[good, song, lyric, prank]</td>\n",
       "      <td>[diving, blue, hole, dahab]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>218</td>\n",
       "      <td>437</td>\n",
       "      <td>438</td>\n",
       "      <td>How do I utilize free time to avoid depression?</td>\n",
       "      <td>Can a person graduating from IIMs like LIK and...</td>\n",
       "      <td>0</td>\n",
       "      <td>[utilize, free, time, avoid, depression]</td>\n",
       "      <td>[person, graduating, iims, lik, iims, ranchi, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>783</td>\n",
       "      <td>1561</td>\n",
       "      <td>1562</td>\n",
       "      <td>How do the PM and Prez get women?</td>\n",
       "      <td>I worked for 1month and absconded from accentu...</td>\n",
       "      <td>0</td>\n",
       "      <td>[pm, prez, women]</td>\n",
       "      <td>[worked, 1month, absconded, accenture, bond, p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>223</td>\n",
       "      <td>447</td>\n",
       "      <td>448</td>\n",
       "      <td>What are some examples of sentences using the ...</td>\n",
       "      <td>What is the best home wireless network setup a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[examples, sentences, word, hysteria]</td>\n",
       "      <td>[best, home, wireless, network, setup, expecte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>233</td>\n",
       "      <td>467</td>\n",
       "      <td>468</td>\n",
       "      <td>Do porn stars watch porn?</td>\n",
       "      <td>How can I book Ronda Rousey to star in an adul...</td>\n",
       "      <td>0</td>\n",
       "      <td>[porn, stars, watch, porn]</td>\n",
       "      <td>[book, ronda, rousey, star, adult, movie]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>235</td>\n",
       "      <td>471</td>\n",
       "      <td>472</td>\n",
       "      <td>Why are we worried about others' opinions?</td>\n",
       "      <td>Why do we care for others' opinion and about w...</td>\n",
       "      <td>1</td>\n",
       "      <td>[worried, opinions]</td>\n",
       "      <td>[care, opinion]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>767</td>\n",
       "      <td>1529</td>\n",
       "      <td>1530</td>\n",
       "      <td>How has the vertebral column anatomy changed t...</td>\n",
       "      <td>When calculating bullet spin; MV X 12 (twist r...</td>\n",
       "      <td>0</td>\n",
       "      <td>[vertebral, column, anatomy, changed, time]</td>\n",
       "      <td>[calculating, bullet, spin, mv, 12, twist, rat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>763</td>\n",
       "      <td>1521</td>\n",
       "      <td>1522</td>\n",
       "      <td>Does any one have ebook of answers of wren and...</td>\n",
       "      <td>Why did barvaria join Germany?</td>\n",
       "      <td>0</td>\n",
       "      <td>[ebook, answers, wren, martin, grammer, compos...</td>\n",
       "      <td>[barvaria, join, germany]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>762</td>\n",
       "      <td>1519</td>\n",
       "      <td>1520</td>\n",
       "      <td>Jawed habib haircut prices?</td>\n",
       "      <td>DO YOU THINK IF MY TREE FALLS ON YOUR PROPERTY...</td>\n",
       "      <td>0</td>\n",
       "      <td>[jawed, habib, haircut, prices]</td>\n",
       "      <td>[tree, falls, property, myine, responsability]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>758</td>\n",
       "      <td>1511</td>\n",
       "      <td>1512</td>\n",
       "      <td>Cochin to London etihad is the change over tim...</td>\n",
       "      <td>What is difference between China and the Unite...</td>\n",
       "      <td>0</td>\n",
       "      <td>[cochin, london, etihad, change, time, 1hr, su...</td>\n",
       "      <td>[difference, china, united, states, send, gifts]</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  qid1  qid2                                          question1  \\\n",
       "999  999  1993  1994               What is a good song for lyric prank?   \n",
       "218  218   437   438    How do I utilize free time to avoid depression?   \n",
       "783  783  1561  1562                  How do the PM and Prez get women?   \n",
       "223  223   447   448  What are some examples of sentences using the ...   \n",
       "233  233   467   468                          Do porn stars watch porn?   \n",
       "235  235   471   472         Why are we worried about others' opinions?   \n",
       "767  767  1529  1530  How has the vertebral column anatomy changed t...   \n",
       "763  763  1521  1522  Does any one have ebook of answers of wren and...   \n",
       "762  762  1519  1520                        Jawed habib haircut prices?   \n",
       "758  758  1511  1512  Cochin to London etihad is the change over tim...   \n",
       "\n",
       "                                             question2  is_duplicate  \\\n",
       "999                     Diving the Blue Hole in Dahab?             0   \n",
       "218  Can a person graduating from IIMs like LIK and...             0   \n",
       "783  I worked for 1month and absconded from accentu...             0   \n",
       "223  What is the best home wireless network setup a...             0   \n",
       "233  How can I book Ronda Rousey to star in an adul...             0   \n",
       "235  Why do we care for others' opinion and about w...             1   \n",
       "767  When calculating bullet spin; MV X 12 (twist r...             0   \n",
       "763                     Why did barvaria join Germany?             0   \n",
       "762  DO YOU THINK IF MY TREE FALLS ON YOUR PROPERTY...             0   \n",
       "758  What is difference between China and the Unite...             0   \n",
       "\n",
       "                                       question1_token  \\\n",
       "999                         [good, song, lyric, prank]   \n",
       "218           [utilize, free, time, avoid, depression]   \n",
       "783                                  [pm, prez, women]   \n",
       "223              [examples, sentences, word, hysteria]   \n",
       "233                         [porn, stars, watch, porn]   \n",
       "235                                [worried, opinions]   \n",
       "767        [vertebral, column, anatomy, changed, time]   \n",
       "763  [ebook, answers, wren, martin, grammer, compos...   \n",
       "762                    [jawed, habib, haircut, prices]   \n",
       "758  [cochin, london, etihad, change, time, 1hr, su...   \n",
       "\n",
       "                                       question2_token  flag_len  cos_sim  \n",
       "999                        [diving, blue, hole, dahab]         0      0.0  \n",
       "218  [person, graduating, iims, lik, iims, ranchi, ...         0      0.0  \n",
       "783  [worked, 1month, absconded, accenture, bond, p...         0      0.0  \n",
       "223  [best, home, wireless, network, setup, expecte...         0      0.0  \n",
       "233          [book, ronda, rousey, star, adult, movie]         0      0.0  \n",
       "235                                    [care, opinion]         0      0.0  \n",
       "767  [calculating, bullet, spin, mv, 12, twist, rat...         0      0.0  \n",
       "763                          [barvaria, join, germany]         0      0.0  \n",
       "762     [tree, falls, property, myine, responsability]         0      0.0  \n",
       "758   [difference, china, united, states, send, gifts]         0      0.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sort_values(by='cos_sim', ascending=True)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
